{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from util import *\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from losses import *\n",
    "from flags_and_variables import *\n",
    "from encode_boxes import *\n",
    "from fcos import *\n",
    "from tensorpack.tfutils.optimizer import AccumGradOptimizer\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1227 17:04:14.557692 140503358441216 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if(FLAGS.use_PANET == True):\n",
    "    pyramid_dict = get_fpn_output(inputs,is_training=False,is_trainable=True,backbone=FLAGS.backbone)\n",
    "    with tf.compat.v1.variable_scope(\"output_PA\") as scope:\n",
    "        #P3+P4\n",
    "        new_P3 = tf.contrib.slim.avg_pool2d(pyramid_dict['P3'],kernel_size=[2,2],stride=2)+pyramid_dict['P4']\n",
    "        new_P3 = tf.layers.conv2d(new_P3, 256, [3,3], 1, 'same', use_bias=True,kernel_initializer=he_normal(seed=1),activation=None,kernel_regularizer=tf.contrib.layers.l2_regularizer(FLAGS.weight_decay))  \n",
    "        #P5+P6+P7\n",
    "        new_P1 = tf.compat.v1.image.resize_bilinear(pyramid_dict['P7'],(pyramid_dict['P6'].shape[1],pyramid_dict['P6'].shape[2]))+pyramid_dict['P6']\n",
    "        new_P1 = tf.layers.conv2d(new_P1, 256, [3,3], 1, 'same', use_bias=True,kernel_initializer=he_normal(seed=1),activation=None,kernel_regularizer=tf.contrib.layers.l2_regularizer(FLAGS.weight_decay))  \n",
    "        new_P1 = tf.compat.v1.image.resize_bilinear(new_P1,(pyramid_dict['P5'].shape[1],pyramid_dict['P5'].shape[2]))+pyramid_dict['P5']\n",
    "        new_P1 = tf.layers.conv2d(new_P1, 256, [3,3], 1, 'same', use_bias=True,kernel_initializer=he_normal(seed=1),activation=None,kernel_regularizer=tf.contrib.layers.l2_regularizer(FLAGS.weight_decay)) \n",
    "        new_P1 = tf.compat.v1.image.resize_bilinear(new_P1,(pyramid_dict['P4'].shape[1],pyramid_dict['P4'].shape[2]))\n",
    "        P = new_P1+new_P3\n",
    "    pyramid_dict = {'P':P}\n",
    "    centerness_output,classes_output,boxes_output = get_network_output(feature_layer_list,pyramid_dict,feature_size,is_training=True)\n",
    "else:\n",
    "    pyramid_dict = get_fpn_output(inputs,is_training=False,is_trainable=True,backbone=FLAGS.backbone)\n",
    "    centerness_output,classes_output,boxes_output = get_network_output(feature_layer_list,pyramid_dict,feature_size,is_training=True)\n",
    "centerness_output = tf.concat(centerness_output,axis=1)\n",
    "classes_output = tf.concat(classes_output,axis=1)\n",
    "boxes_output = tf.concat(boxes_output,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training part\n",
    "centerness_output_test = centerness_output\n",
    "classes_output_test = classes_output\n",
    "boxes_output_test = boxes_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###testing part\n",
    "centerness_pred = tf.sigmoid(centerness_output)\n",
    "classes_pred = tf.sigmoid(classes_output)\n",
    "localization_pred = boxes_output\n",
    "if(FLAGS.use_PANET==True):\n",
    "    _scores,_classes,_boxes = predict_outputs_PANET(centerness_pred,classes_pred,localization_pred,feature_size,stride)\n",
    "else:\n",
    "    _scores,_classes,_boxes = predict_outputs(centerness_pred,classes_pred,localization_pred,feature_size,stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#focal loss\n",
    "focal_losses = focal_loss(classes,classes_output_test,state)\n",
    "IOU_losses = iou_loss(boxes, boxes_output_test,state,centerness)\n",
    "centerness_losses = centerness_loss(centerness, centerness_output_test,state)\n",
    "regular_loss = tf.losses.get_regularization_loss()\n",
    "total_losses = focal_losses+IOU_losses+centerness_losses+regular_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1227 17:04:18 @gradproc.py:79]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m No gradient w.r.t 3 trainable variables: resnet_v2_50/postnorm/gamma, resnet_v2_50/postnorm/beta, resnet_v2_50/logits/biases\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1227 17:04:19.149057 140503358441216 module_wrapper.py:139] From /opt/anaconda3/lib/python3.7/site-packages/tensorpack/tfutils/optimizer.py:201: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W1227 17:04:19.152717 140503358441216 module_wrapper.py:139] From /opt/anaconda3/lib/python3.7/site-packages/tensorpack/tfutils/optimizer.py:210: The name tf.mod is deprecated. Please use tf.math.mod instead.\n",
      "\n",
      "W1227 17:04:21.640163 140503358441216 module_wrapper.py:139] From /opt/anaconda3/lib/python3.7/site-packages/tensorpack/tfutils/optimizer.py:207: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.summary.scalar('scala/total_losses',total_losses)\n",
    "tf.summary.scalar('scala/focal_losses',focal_losses)\n",
    "tf.summary.scalar('scala/IOU_losses',IOU_losses)\n",
    "tf.summary.scalar('scala/centerness_losses',centerness_losses)\n",
    "tf.summary.scalar('scala/reg_loss',regular_loss)\n",
    "ckpt = tf.train.latest_checkpoint(FLAGS.checkpoint_dir)\n",
    "if ckpt:\n",
    "    start_step = int(ckpt.split('-')[-1])\n",
    "else:\n",
    "    start_step = 0  \n",
    "    \n",
    "with tf.variable_scope(\"optimizer\") as scope:\n",
    "    global_step = tf.get_variable(\"step\", [], initializer=tf.constant_initializer(start_step), trainable=False)\n",
    "#     rate = tf.train.exponential_decay(FLAGS.learning_rate, global_step, decay_steps=20000, decay_rate=0.1, staircase=True)\n",
    "    rate = FLAGS.learning_rate\n",
    "    update_opts = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies([tf.group(update_opts)]):\n",
    "        adam_opt = tf.train.AdamOptimizer(rate)\n",
    "#         momentum_opt = tf.train.MomentumOptimizer(rate,momentum=0.9)\n",
    "#         SGD_opt = tf.train.GradientDescentOptimizer(learning_rate=rate)\n",
    "        train_opt = AccumGradOptimizer(adam_opt, niter=iter_size).minimize(total_losses,global_step=global_step)\n",
    "tf.summary.scalar('scala/learning_rate',rate)\n",
    "merged_summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of training samples are:14568\n"
     ]
    }
   ],
   "source": [
    "annotation_2012_train = get_path_and_annotation(FLAGS.train_2012_dir)\n",
    "annotation_2007_train = get_path_and_annotation(FLAGS.train_2007_dir)\n",
    "extra_annotation = get_path_and_annotation('/media/xinje/New Volume/VOC07&12/VOC2012/txt/diningtable.txt')+get_path_and_annotation('/media/xinje/New Volume/VOC07&12/VOC2007/train/txt/diningtable.txt')\n",
    "extra_annotation = extra_annotation+get_path_and_annotation('/media/xinje/New Volume/VOC07&12/VOC2012/txt/pottedplant.txt')+get_path_and_annotation('/media/xinje/New Volume/VOC07&12/VOC2007/train/txt/pottedplant.txt')\n",
    "annotation_train = annotation_2012_train+annotation_2007_train+extra_annotation*5\n",
    "random.shuffle(annotation_train)\n",
    "\n",
    "annotation_2012_test = get_path_and_annotation(FLAGS.test_2012_dir)\n",
    "annotation_2007_test = get_path_and_annotation(FLAGS.test_2007_dir)\n",
    "annotation_test = annotation_2012_test +annotation_2007_test\n",
    "print('The number of training samples are:'+str(len(annotation_train)))\n",
    "train_iter = read_data(annotation_train,FLAGS.batch_size,input_shape=(FLAGS.image_height,FLAGS.image_width),is_random=True,is_crop=False)\n",
    "test_iter = read_data(annotation_test,1,input_shape=(FLAGS.image_height,FLAGS.image_width),is_random=False,is_crop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restore from the checkpoint /media/xinje/New Volume/fcos/resnet_v2_50_freeze_bn/FCOS-resnet_v2_50-3050\n",
      "Batch: 3051   Current total loss: 1.1667099 focal_loss: 0.11996215 IOU_loss: 0.31207117 centerness_loss: 0.5891756\n",
      "Batch: 3052   Current total loss: 1.2138743 focal_loss: 0.14727879 IOU_loss: 0.32377818 centerness_loss: 0.5973218\n",
      "Batch: 3053   Current total loss: 1.4384216 focal_loss: 0.22103383 IOU_loss: 0.46210113 centerness_loss: 0.6097963\n",
      "Batch: 3054   Current total loss: 1.3758441 focal_loss: 0.28362462 IOU_loss: 0.35420957 centerness_loss: 0.5925246\n",
      "Batch: 3055   Current total loss: 2.3927157 focal_loss: 0.711711 IOU_loss: 0.89895636 centerness_loss: 0.6365671\n",
      "Batch: 3056   Current total loss: 1.4898269 focal_loss: 0.39462924 IOU_loss: 0.34568468 centerness_loss: 0.6040347\n",
      "Batch: 3057   Current total loss: 1.6810908 focal_loss: 0.39346942 IOU_loss: 0.52161473 centerness_loss: 0.6205305\n",
      "Batch: 3058   Current total loss: 1.9376361 focal_loss: 0.5065645 IOU_loss: 0.6535665 centerness_loss: 0.63203007\n",
      "Batch: 3059   Current total loss: 1.5477464 focal_loss: 0.32270142 IOU_loss: 0.46264082 centerness_loss: 0.6169295\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    train_writer = tf.summary.FileWriter(FLAGS.log_Dir,sess.graph)\n",
    "#     var_list = [var for var in tf.global_variables() if ('optimizer') not in var.name]\n",
    "    var_list_final = tf.global_variables()\n",
    "    saver_final = tf.train.Saver(var_list=var_list_final,max_to_keep=5)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    ckpt = tf.train.latest_checkpoint(FLAGS.checkpoint_dir)\n",
    "    if ckpt:\n",
    "        saver_final.restore(sess,ckpt)\n",
    "        print('Restore from the checkpoint {0}'.format(ckpt)) \n",
    "        ####\n",
    "#         exclude = [\"optimizer/resnet_v2_50/\"]\n",
    "#         variables_to_restore = tf.contrib.slim.get_variables_to_restore(exclude=exclude)\n",
    "#         saver = tf.train.Saver(var_list=variables_to_restore,max_to_keep=5)\n",
    "#         saver.restore(sess,ckpt)\n",
    "\n",
    "    else:\n",
    "        print('Train ssd from start')\n",
    "        if(FLAGS.backbone == 'resnet_v2_50'):\n",
    "            ckpt = './resnet_v2_50_2017_04_14/resnet_v2_50.ckpt'\n",
    "            exclude = ['resnet_v2_50' + '/logits', 'global_step','output','output2',\"optimizer\",'postnorm','output_PA']\n",
    "        if(FLAGS.backbone == 'vgg_16'):\n",
    "            ckpt = './vgg16/vgg_16.ckpt'\n",
    "            exclude = ['vgg_16' + '/logits', 'global_step','output','output2',\"optimizer\",'postnorm','output_PA']\n",
    "        variables_to_restore = tf.contrib.slim.get_variables_to_restore(exclude=exclude)\n",
    "        saver = tf.train.Saver(var_list=variables_to_restore,max_to_keep=5)\n",
    "        saver.restore(sess,ckpt)\n",
    "        \n",
    "    for iteration in range(start_step+1,10000000):\n",
    "        #shuffle the whole dataset every epoch\n",
    "        if(iteration%(len(annotation_train)//FLAGS.batch_size)==0):\n",
    "            random.shuffle(annotation_train)\n",
    "            train_iter = read_data(annotation_train,FLAGS.batch_size,input_shape=(FLAGS.image_height,FLAGS.image_width),is_random=True,is_crop=False)\n",
    "        image_batch,annotation_batch,cls_batch = next(train_iter)\n",
    "        #one hot for cls_batch(change class from[1,20] to [0,21])\n",
    "        eye = np.eye(FLAGS.num_class)\n",
    "        eye = np.concatenate([eye,np.expand_dims(np.zeros_like(eye[0]),axis=0)])\n",
    "        cls_batch = eye[(cls_batch-1).astype(np.int32)].astype(np.float32)\n",
    "        if(FLAGS.use_PANET==True):\n",
    "            matched_true_boxes,matched_true_classes,matched_true_centerness = encode_boxes_PANET(annotation_batch,cls_batch,feature_size,stride)\n",
    "        else:\n",
    "            matched_true_boxes,matched_true_classes,matched_true_centerness = encode_boxes(annotation_batch,cls_batch,feature_size,stride)\n",
    "        feature_state = np.squeeze(matched_true_centerness!=0,axis=-1)\n",
    "        feed_dict={inputs:image_batch,boxes:matched_true_boxes,classes:matched_true_classes,centerness:matched_true_centerness,state:feature_state}\n",
    "        [_,train_summary,temp_loss,temp_focal_loss,temp_iou_loss,temp_centerness_loss,temp_centerness_output,temp_classes_output,temp_boxes_output] = sess.run([train_opt,merged_summary_op,total_losses,focal_losses,IOU_losses,centerness_losses,centerness_output_test,classes_output_test,boxes_output_test],feed_dict=feed_dict)\n",
    "        \n",
    "        real_step = (iteration-start_step)//iter_size+start_step\n",
    "        if((iteration-start_step)%iter_size==0):\n",
    "            print('Batch: '+str(real_step)+'   Current total loss: '+str(temp_loss)+' focal_loss: '+str(temp_focal_loss)+' IOU_loss: '+str(temp_iou_loss)+' centerness_loss: '+str(temp_centerness_loss))\n",
    "            train_writer.add_summary(train_summary, real_step)\n",
    "        #save checkpoint and inference one image    \n",
    "        if((iteration-start_step)%(iter_size*50)==0 and iteration!=0):\n",
    "            for inference_iter in range(10):\n",
    "                image_batch,annotation_batch,cls_batch = next(test_iter)\n",
    "                [temp_scores_pred_list,temp_classes_pred_list,temp_localization_pred_list] = sess.run([_scores,_classes,_boxes],feed_dict={inputs:image_batch})\n",
    "                ymin,xmin,ymax,xmax=np.split(temp_localization_pred_list,axis=1,indices_or_sections =4)\n",
    "                ymax[ymax>FLAGS.image_height]=FLAGS.image_height\n",
    "                xmax[xmax>FLAGS.image_width]=FLAGS.image_width\n",
    "                ymin[ymin<0]=0\n",
    "                xmin[xmin<0]=0\n",
    "                plt.hlines(ymin,xmin,xmax,'r')\n",
    "                plt.hlines(ymax,xmin,xmax,'r')\n",
    "                plt.vlines(xmin,ymin,ymax,'r')\n",
    "                plt.vlines(xmax,ymin,ymax,'r')\n",
    "                for j in range(temp_classes_pred_list.shape[0]):\n",
    "                    position = (ymin[j],xmin[j])\n",
    "                    plt.text(position[1]+10,position[0]+10,corresponding_dict[temp_classes_pred_list[j]],color='g',size = 10)\n",
    "                plt.imshow(image_batch[0])\n",
    "                plt.show()\n",
    "#         save model for training\n",
    "            saver_final.save(sess,os.path.join(FLAGS.checkpoint_dir,'FCOS-'+FLAGS.backbone),global_step=real_step) \n",
    "            #########################save model for inference#############################################\n",
    "#             constant_graph = convert_variables_to_constants(sess, sess.graph_def, ['boxes','scores','classes'])\n",
    "#             with tf.gfile.FastGFile('./model.pb', mode='wb') as f:\n",
    "#                 f.write(constant_graph.SerializeToString())\n",
    "            #####################################################################\n",
    "            print('Model saved !!!')         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# learning_rate = 1e-2\n",
    "# Si = tf.Variable(tf.constant(1.0,shape=[5,1],dtype=tf.float32), trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matched_true_boxes,matched_true_classes,matched_true_centerness = encode_boxes(annotation_batch,cls_batch,feature_size,stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(y_true, y_pred,anchor_state):\n",
    "    \"\"\"\n",
    "    Compute the focal loss given the target tensor and the predicted tensor.\n",
    "    As defined in https://arxiv.org/abs/1708.02002\n",
    "    Args\n",
    "        y_true: Tensor of target data from the generator with shape (B, N, num_classes).\n",
    "        y_pred: Tensor of predicted data from the network with shape (B, N, num_classes).\n",
    "    Returns\n",
    "        The focal loss of y_pred w.r.t. y_true.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(\"loss/focal\") as scope:\n",
    "        alpha=0.25\n",
    "        gamma=2.0\n",
    "        # compute the focal loss\n",
    "        location_state = anchor_state\n",
    "        labels = y_true\n",
    "        # alpha 参与用于调节正负样本的平衡问题\n",
    "        alpha_factor = tf.ones_like(labels) * alpha\n",
    "        alpha_factor = tf.where(tf.equal(labels, 1), alpha_factor, 1 - alpha_factor)\n",
    "        # focal_weight 用来使置信度较高容易的样本的 loss 比原来小, 而置信度低的难度大的 loss 变化不大\n",
    "        # (1 - 0.99) ** 2 = 1e-4, (1 - 0.9) ** 2 = 1e-2\n",
    "        focal_weight = tf.where(tf.equal(labels, 1), 1 - y_pred, y_pred)\n",
    "        focal_weight = alpha_factor * focal_weight ** gamma\n",
    "        # binary_crossentropy 是  -log(p) y=1 -log(1-p) y=others, 那么论文中统一的用 -log(pt) 来表示\n",
    "        #cls_loss = focal_weight * tf.nn.sigmoid_cross_entropy_with_logits(labels=labels,logits=y_pred)\n",
    "        cls_loss = focal_weight * tf.keras.backend.binary_crossentropy(target=labels,output=y_pred)\n",
    "        # compute the normalizer: the number of positive anchors\n",
    "        normalizer = tf.where(tf.equal(location_state, 1))\n",
    "        normalizer = tf.cast(tf.shape(normalizer)[0], tf.float32)\n",
    "        normalizer = tf.maximum(1.0, normalizer)\n",
    "        loss = tf.reduce_sum(cls_loss) / normalizer\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def iou_loss(y_true, y_pred,location_state,centerness):\n",
    "    with tf.variable_scope(\"loss/iou\") as scope:\n",
    "        # pos location\n",
    "        indices = tf.where(tf.equal(location_state, 1))\n",
    "    #     print(indices.shape)\n",
    "        if tf.size(indices) == 0:\n",
    "            return tf.constant(0.0)\n",
    "        y_regr_pred = tf.gather_nd(y_pred, indices)\n",
    "    #     print(y_regr_pred.shape)\n",
    "        y_regr_true = tf.gather_nd(y_true, indices)\n",
    "        y_centerness_true = tf.gather_nd(centerness,indices)\n",
    "\n",
    "        # (num_pos, )\n",
    "        pred_top = y_regr_pred[:, 0]\n",
    "        pred_bottom = y_regr_pred[:, 1]\n",
    "        pred_left = y_regr_pred[:, 2]\n",
    "        pred_right = y_regr_pred[:, 3]\n",
    "\n",
    "        # (num_pos, )\n",
    "        target_top = y_regr_true[:, 0]\n",
    "        target_bottom = y_regr_true[:, 1]\n",
    "        target_left = y_regr_true[:, 2]\n",
    "        target_right = y_regr_true[:, 3]\n",
    "\n",
    "        target_area = (target_left + target_right) * (target_top + target_bottom)\n",
    "        pred_area = (pred_left + pred_right) * (pred_top + pred_bottom)\n",
    "        w_intersect = tf.minimum(pred_left, target_left) + tf.minimum(pred_right, target_right)\n",
    "        h_intersect = tf.minimum(pred_bottom, target_bottom) + tf.minimum(pred_top, target_top)\n",
    "\n",
    "        g_w_intersect = tf.maximum(pred_left, target_left) + tf.maximum(pred_right, target_right)\n",
    "        g_h_intersect = tf.maximum(pred_bottom, target_bottom) + tf.maximum(pred_top, target_top)\n",
    "        ac_union = g_w_intersect * g_h_intersect\n",
    "        euclidean_distance = tf.pow((-target_left + target_right)/2 -(-pred_left + pred_right)/2,2) + tf.pow((-target_top + target_bottom)/2 -(-pred_top + pred_bottom)/2,2)\n",
    "        area_intersect = w_intersect * h_intersect\n",
    "        area_union = target_area + pred_area - area_intersect\n",
    "        ious = (area_intersect) / (area_union+1e-8)\n",
    "        dious = 1-ious+(euclidean_distance/tf.pow(ac_union+1e-8,2))\n",
    "        # (num_pos, )\n",
    "        #losses = tf.reduce_sum(losses * y_centerness_true) / (tf.reduce_sum(y_centerness_true) + 1e-6)\n",
    "\n",
    "        losses =  tf.reduce_mean(dious)\n",
    "    return losses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
