{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from util import *\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from losses import *\n",
    "from flags_and_variables import *\n",
    "from encode_boxes import *\n",
    "from fcos import *\n",
    "from tensorpack.tfutils.optimizer import AccumGradOptimizer\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1226 14:46:49.392436 139955073693440 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pyramid_dict = get_fpn_output(inputs,is_training=False,is_trainable=True,backbone=FLAGS.backbone)\n",
    "centerness_output,classes_output,boxes_output = get_network_output(feature_layer_list,pyramid_dict,feature_size,is_training=True)\n",
    "centerness_output = tf.concat(centerness_output,axis=1)\n",
    "classes_output = tf.concat(classes_output,axis=1)\n",
    "boxes_output = tf.concat(boxes_output,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training part\n",
    "centerness_output_test = centerness_output\n",
    "classes_output_test = classes_output\n",
    "boxes_output_test = boxes_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###testing part\n",
    "centerness_pred = tf.sigmoid(centerness_output)\n",
    "classes_pred = tf.sigmoid(classes_output)\n",
    "localization_pred = boxes_output\n",
    "_scores,_classes,_boxes = predict_outputs(centerness_pred,classes_pred,localization_pred,feature_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#focal loss\n",
    "focal_losses = focal_loss(classes,classes_output_test,state)\n",
    "IOU_losses = iou_loss(boxes, boxes_output_test,state,centerness)\n",
    "centerness_losses = centerness_loss(centerness, centerness_output_test,state)\n",
    "regular_loss = tf.losses.get_regularization_loss()\n",
    "total_losses = focal_losses+IOU_losses+centerness_losses+regular_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1226 14:46:53 @gradproc.py:79]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m No gradient w.r.t 3 trainable variables: resnet_v2_50/postnorm/gamma, resnet_v2_50/postnorm/beta, resnet_v2_50/logits/biases\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1226 14:46:54.050965 139955073693440 module_wrapper.py:139] From /opt/anaconda3/lib/python3.7/site-packages/tensorpack/tfutils/optimizer.py:201: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W1226 14:46:54.056241 139955073693440 module_wrapper.py:139] From /opt/anaconda3/lib/python3.7/site-packages/tensorpack/tfutils/optimizer.py:210: The name tf.mod is deprecated. Please use tf.math.mod instead.\n",
      "\n",
      "W1226 14:46:56.693658 139955073693440 module_wrapper.py:139] From /opt/anaconda3/lib/python3.7/site-packages/tensorpack/tfutils/optimizer.py:207: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.summary.scalar('scala/total_losses',total_losses)\n",
    "tf.summary.scalar('scala/focal_losses',focal_losses)\n",
    "tf.summary.scalar('scala/IOU_losses',IOU_losses)\n",
    "tf.summary.scalar('scala/centerness_losses',centerness_losses)\n",
    "tf.summary.scalar('scala/reg_loss',regular_loss)\n",
    "ckpt = tf.train.latest_checkpoint(FLAGS.checkpoint_dir)\n",
    "if ckpt:\n",
    "    start_step = int(ckpt.split('-')[-1])\n",
    "else:\n",
    "    start_step = 0  \n",
    "    \n",
    "with tf.variable_scope(\"optimizer\") as scope:\n",
    "    global_step = tf.get_variable(\"step\", [], initializer=tf.constant_initializer(start_step), trainable=False)\n",
    "#     rate = tf.train.exponential_decay(FLAGS.learning_rate, global_step, decay_steps=20000, decay_rate=0.1, staircase=True)\n",
    "    rate = FLAGS.learning_rate\n",
    "    update_opts = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies([tf.group(update_opts)]):\n",
    "        adam_opt = tf.train.AdamOptimizer(rate)\n",
    "#         momentum_opt = tf.train.MomentumOptimizer(rate,momentum=0.9)\n",
    "#         SGD_opt = tf.train.GradientDescentOptimizer(learning_rate=rate)\n",
    "        train_opt = AccumGradOptimizer(adam_opt, niter=iter_size).minimize(total_losses,global_step=global_step)\n",
    "tf.summary.scalar('scala/learning_rate',rate)\n",
    "merged_summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_2012_train = get_path_and_annotation(FLAGS.train_2012_dir)\n",
    "annotation_2007_train = get_path_and_annotation(FLAGS.train_2007_dir)\n",
    "extra_annotation = get_path_and_annotation('/media/xinje/New Volume/VOC07&12/VOC2012/txt/diningtable.txt')+get_path_and_annotation('/media/xinje/New Volume/VOC07&12/VOC2007/train/txt/diningtable.txt')\n",
    "extra_annotation = extra_annotation+get_path_and_annotation('/media/xinje/New Volume/VOC07&12/VOC2012/txt/pottedplant.txt')+get_path_and_annotation('/media/xinje/New Volume/VOC07&12/VOC2007/train/txt/pottedplant.txt')\n",
    "annotation_train = extra_annotation+annotation_2012_train +annotation_2007_train+extra_annotation+extra_annotation+extra_annotation\n",
    "random.shuffle(annotation_train)\n",
    "\n",
    "annotation_2012_test = get_path_and_annotation(FLAGS.test_2012_dir)\n",
    "annotation_2007_test = get_path_and_annotation(FLAGS.test_2007_dir)\n",
    "annotation_test = annotation_2012_test +annotation_2007_test\n",
    "\n",
    "train_iter = read_data(annotation_train,FLAGS.batch_size,input_shape=(FLAGS.image_height,FLAGS.image_width),is_random=True,is_crop=False)\n",
    "test_iter = read_data(annotation_test,1,input_shape=(FLAGS.image_height,FLAGS.image_width),is_random=False,is_crop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restore from the checkpoint /media/xinje/New Volume/fcos/resnet_v2_50_freeze_bn/FCOS-resnet_v2_50-1800\n",
      "Batch: 1801   Current total loss: 1.3412596 focal_loss: 0.17494665 IOU_loss: 0.4058574 centerness_loss: 0.6142973\n",
      "Batch: 1802   Current total loss: 1.2178963 focal_loss: 0.14690647 IOU_loss: 0.3441132 centerness_loss: 0.5807189\n",
      "Batch: 1803   Current total loss: 1.5428554 focal_loss: 0.28386003 IOU_loss: 0.49650988 centerness_loss: 0.6163281\n",
      "Batch: 1804   Current total loss: 1.4403592 focal_loss: 0.19532725 IOU_loss: 0.48050746 centerness_loss: 0.61836755\n",
      "Batch: 1805   Current total loss: 1.2477587 focal_loss: 0.1564957 IOU_loss: 0.3487083 centerness_loss: 0.59639835\n",
      "Batch: 1806   Current total loss: 1.3794206 focal_loss: 0.208018 IOU_loss: 0.42800826 centerness_loss: 0.5972384\n",
      "Batch: 1807   Current total loss: 1.3649195 focal_loss: 0.18653698 IOU_loss: 0.42489523 centerness_loss: 0.60733193\n",
      "Batch: 1808   Current total loss: 1.3591945 focal_loss: 0.18746565 IOU_loss: 0.4220405 centerness_loss: 0.6035334\n",
      "Batch: 1809   Current total loss: 1.586363 focal_loss: 0.19666632 IOU_loss: 0.6023023 centerness_loss: 0.6412399\n",
      "Batch: 1810   Current total loss: 1.2912152 focal_loss: 0.1968549 IOU_loss: 0.35079724 centerness_loss: 0.597409\n",
      "Batch: 1811   Current total loss: 1.4037489 focal_loss: 0.22048298 IOU_loss: 0.43075383 centerness_loss: 0.6063586\n",
      "Batch: 1812   Current total loss: 1.2973342 focal_loss: 0.15444787 IOU_loss: 0.39254314 centerness_loss: 0.6041901\n",
      "Batch: 1813   Current total loss: 1.5736135 focal_loss: 0.21007335 IOU_loss: 0.5995553 centerness_loss: 0.6178323\n",
      "Batch: 1814   Current total loss: 1.2783602 focal_loss: 0.18390927 IOU_loss: 0.35580587 centerness_loss: 0.5924929\n",
      "Batch: 1815   Current total loss: 1.2809469 focal_loss: 0.15136324 IOU_loss: 0.3822667 centerness_loss: 0.60116524\n",
      "Batch: 1816   Current total loss: 1.1836026 focal_loss: 0.17002688 IOU_loss: 0.28244314 centerness_loss: 0.5849814\n",
      "Batch: 1817   Current total loss: 1.2653552 focal_loss: 0.17589459 IOU_loss: 0.34433055 centerness_loss: 0.5989793\n",
      "Batch: 1818   Current total loss: 1.2791489 focal_loss: 0.2442694 IOU_loss: 0.28949115 centerness_loss: 0.5992382\n",
      "Batch: 1819   Current total loss: 1.3372699 focal_loss: 0.21175322 IOU_loss: 0.38479114 centerness_loss: 0.5945758\n",
      "Batch: 1820   Current total loss: 1.1862549 focal_loss: 0.17133994 IOU_loss: 0.29631084 centerness_loss: 0.57245475\n",
      "Batch: 1821   Current total loss: 1.2525779 focal_loss: 0.20376265 IOU_loss: 0.31262773 centerness_loss: 0.59003866\n",
      "Batch: 1822   Current total loss: 1.3382622 focal_loss: 0.19885017 IOU_loss: 0.39164793 centerness_loss: 0.6016157\n",
      "Batch: 1823   Current total loss: 1.1790054 focal_loss: 0.14371884 IOU_loss: 0.28825006 centerness_loss: 0.60088867\n",
      "Batch: 1824   Current total loss: 1.3213843 focal_loss: 0.2323151 IOU_loss: 0.33815703 centerness_loss: 0.6047649\n",
      "Batch: 1825   Current total loss: 1.2593437 focal_loss: 0.17872185 IOU_loss: 0.33688194 centerness_loss: 0.597593\n",
      "Batch: 1826   Current total loss: 1.2377747 focal_loss: 0.2037325 IOU_loss: 0.29880124 centerness_loss: 0.5890945\n",
      "Batch: 1827   Current total loss: 1.2510413 focal_loss: 0.13299032 IOU_loss: 0.38207883 centerness_loss: 0.5898263\n",
      "Batch: 1828   Current total loss: 1.3035072 focal_loss: 0.18169841 IOU_loss: 0.375271 centerness_loss: 0.6003924\n",
      "Batch: 1829   Current total loss: 1.3269159 focal_loss: 0.2182016 IOU_loss: 0.36783603 centerness_loss: 0.5947332\n",
      "Batch: 1830   Current total loss: 1.3436477 focal_loss: 0.16989452 IOU_loss: 0.42454222 centerness_loss: 0.60306644\n",
      "Batch: 1831   Current total loss: 1.3319494 focal_loss: 0.1852353 IOU_loss: 0.3985797 centerness_loss: 0.6019903\n",
      "Batch: 1832   Current total loss: 1.2811193 focal_loss: 0.20677051 IOU_loss: 0.3396604 centerness_loss: 0.5885449\n",
      "Batch: 1833   Current total loss: 1.2653284 focal_loss: 0.17356119 IOU_loss: 0.3361198 centerness_loss: 0.6095044\n",
      "Batch: 1834   Current total loss: 1.5133364 focal_loss: 0.22115985 IOU_loss: 0.53215504 centerness_loss: 0.61387914\n",
      "Batch: 1835   Current total loss: 1.2655677 focal_loss: 0.16973543 IOU_loss: 0.3407328 centerness_loss: 0.60895723\n",
      "Batch: 1836   Current total loss: 1.260713 focal_loss: 0.16894242 IOU_loss: 0.34434298 centerness_loss: 0.601286\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    train_writer = tf.summary.FileWriter(FLAGS.log_Dir,sess.graph)\n",
    "#     var_list = [var for var in tf.global_variables() if ('optimizer') not in var.name]\n",
    "    var_list_final = tf.global_variables()\n",
    "    saver_final = tf.train.Saver(var_list=var_list_final,max_to_keep=5)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    ckpt = tf.train.latest_checkpoint(FLAGS.checkpoint_dir)\n",
    "    if ckpt:\n",
    "        saver_final.restore(sess,ckpt)\n",
    "        print('Restore from the checkpoint {0}'.format(ckpt)) \n",
    "        ####\n",
    "#         exclude = [\"optimizer/resnet_v2_50/\"]\n",
    "#         variables_to_restore = tf.contrib.slim.get_variables_to_restore(exclude=exclude)\n",
    "#         saver = tf.train.Saver(var_list=variables_to_restore,max_to_keep=5)\n",
    "#         saver.restore(sess,ckpt)\n",
    "\n",
    "    else:\n",
    "        print('Train ssd from start')\n",
    "        if(FLAGS.backbone == 'resnet_v2_50'):\n",
    "            ckpt = './resnet_v2_50_2017_04_14/resnet_v2_50.ckpt'\n",
    "            exclude = ['resnet_v2_50' + '/logits', 'global_step','output','output2',\"optimizer\",'postnorm']\n",
    "        if(FLAGS.backbone == 'vgg_16'):\n",
    "            ckpt = './vgg16/vgg_16.ckpt'\n",
    "            exclude = ['vgg_16' + '/logits', 'global_step','output','output2',\"optimizer\",'postnorm']\n",
    "        variables_to_restore = tf.contrib.slim.get_variables_to_restore(exclude=exclude)\n",
    "        saver = tf.train.Saver(var_list=variables_to_restore,max_to_keep=5)\n",
    "        saver.restore(sess,ckpt)\n",
    "        \n",
    "    for iteration in range(start_step+1,10000000):\n",
    "        #shuffle the whole dataset every epoch\n",
    "        if(iteration%(len(annotation_train)//FLAGS.batch_size)==0):\n",
    "            random.shuffle(annotation_train)\n",
    "            train_iter = read_data(annotation_train,FLAGS.batch_size,input_shape=(FLAGS.image_height,FLAGS.image_width),is_random=True,is_crop=False)\n",
    "        image_batch,annotation_batch,cls_batch = next(train_iter)\n",
    "        #one hot for cls_batch(change class from[1,20] to [0,21])\n",
    "        eye = np.eye(FLAGS.num_class)\n",
    "        eye = np.concatenate([eye,np.expand_dims(np.zeros_like(eye[0]),axis=0)])\n",
    "        cls_batch = eye[(cls_batch-1).astype(np.int32)].astype(np.float32)\n",
    "        \n",
    "        matched_true_boxes,matched_true_classes,matched_true_centerness = encode_boxes(annotation_batch,cls_batch,feature_size,stride)\n",
    "        feature_state = np.squeeze(matched_true_centerness!=0,axis=-1)\n",
    "        feed_dict={inputs:image_batch,boxes:matched_true_boxes,classes:matched_true_classes,centerness:matched_true_centerness,state:feature_state}\n",
    "        [_,train_summary,temp_loss,temp_focal_loss,temp_iou_loss,temp_centerness_loss,temp_centerness_output,temp_classes_output,temp_boxes_output] = sess.run([train_opt,merged_summary_op,total_losses,focal_losses,IOU_losses,centerness_losses,centerness_output_test,classes_output_test,boxes_output_test],feed_dict=feed_dict)\n",
    "        \n",
    "        real_step = (iteration-start_step)//iter_size+start_step\n",
    "        if((iteration-start_step)%iter_size==0):\n",
    "            print('Batch: '+str(real_step)+'   Current total loss: '+str(temp_loss)+' focal_loss: '+str(temp_focal_loss)+' IOU_loss: '+str(temp_iou_loss)+' centerness_loss: '+str(temp_centerness_loss))\n",
    "            train_writer.add_summary(train_summary, real_step)\n",
    "        #save checkpoint and inference one image    \n",
    "        if((iteration-start_step)%(iter_size*50)==0 and iteration!=0):\n",
    "            for inference_iter in range(10):\n",
    "                image_batch,annotation_batch,cls_batch = next(test_iter)\n",
    "                [temp_scores_pred_list,temp_classes_pred_list,temp_localization_pred_list] = sess.run([_scores,_classes,_boxes],feed_dict={inputs:image_batch})\n",
    "                ymin,xmin,ymax,xmax=np.split(temp_localization_pred_list,axis=1,indices_or_sections =4)\n",
    "                ymax[ymax>FLAGS.image_height]=FLAGS.image_height\n",
    "                xmax[xmax>FLAGS.image_width]=FLAGS.image_width\n",
    "                ymin[ymin<0]=0\n",
    "                xmin[xmin<0]=0\n",
    "                plt.hlines(ymin,xmin,xmax,'r')\n",
    "                plt.hlines(ymax,xmin,xmax,'r')\n",
    "                plt.vlines(xmin,ymin,ymax,'r')\n",
    "                plt.vlines(xmax,ymin,ymax,'r')\n",
    "                for j in range(temp_classes_pred_list.shape[0]):\n",
    "                    position = (ymin[j],xmin[j])\n",
    "                    plt.text(position[1]+10,position[0]+10,corresponding_dict[temp_classes_pred_list[j]],color='g',size = 10)\n",
    "                plt.imshow(image_batch[0])\n",
    "                plt.show()\n",
    "#         save model for training\n",
    "            saver_final.save(sess,os.path.join(FLAGS.checkpoint_dir,'FCOS-'+FLAGS.backbone),global_step=real_step) \n",
    "            #########################save model for inference#############################################\n",
    "#             constant_graph = convert_variables_to_constants(sess, sess.graph_def, ['boxes','scores','classes'])\n",
    "#             with tf.gfile.FastGFile('./model.pb', mode='wb') as f:\n",
    "#                 f.write(constant_graph.SerializeToString())\n",
    "            #####################################################################\n",
    "            print('Model saved !!!')         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# learning_rate = 1e-2\n",
    "# Si = tf.Variable(tf.constant(1.0,shape=[5,1],dtype=tf.float32), trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matched_true_boxes,matched_true_classes,matched_true_centerness = encode_boxes(annotation_batch,cls_batch,feature_size,stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(y_true, y_pred,anchor_state):\n",
    "    \"\"\"\n",
    "    Compute the focal loss given the target tensor and the predicted tensor.\n",
    "    As defined in https://arxiv.org/abs/1708.02002\n",
    "    Args\n",
    "        y_true: Tensor of target data from the generator with shape (B, N, num_classes).\n",
    "        y_pred: Tensor of predicted data from the network with shape (B, N, num_classes).\n",
    "    Returns\n",
    "        The focal loss of y_pred w.r.t. y_true.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(\"loss/focal\") as scope:\n",
    "        alpha=0.25\n",
    "        gamma=2.0\n",
    "        # compute the focal loss\n",
    "        location_state = anchor_state\n",
    "        labels = y_true\n",
    "        # alpha 参与用于调节正负样本的平衡问题\n",
    "        alpha_factor = tf.ones_like(labels) * alpha\n",
    "        alpha_factor = tf.where(tf.equal(labels, 1), alpha_factor, 1 - alpha_factor)\n",
    "        # focal_weight 用来使置信度较高容易的样本的 loss 比原来小, 而置信度低的难度大的 loss 变化不大\n",
    "        # (1 - 0.99) ** 2 = 1e-4, (1 - 0.9) ** 2 = 1e-2\n",
    "        focal_weight = tf.where(tf.equal(labels, 1), 1 - y_pred, y_pred)\n",
    "        focal_weight = alpha_factor * focal_weight ** gamma\n",
    "        # binary_crossentropy 是  -log(p) y=1 -log(1-p) y=others, 那么论文中统一的用 -log(pt) 来表示\n",
    "        #cls_loss = focal_weight * tf.nn.sigmoid_cross_entropy_with_logits(labels=labels,logits=y_pred)\n",
    "        cls_loss = focal_weight * tf.keras.backend.binary_crossentropy(target=labels,output=y_pred)\n",
    "        # compute the normalizer: the number of positive anchors\n",
    "        normalizer = tf.where(tf.equal(location_state, 1))\n",
    "        normalizer = tf.cast(tf.shape(normalizer)[0], tf.float32)\n",
    "        normalizer = tf.maximum(1.0, normalizer)\n",
    "        loss = tf.reduce_sum(cls_loss) / normalizer\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def iou_loss(y_true, y_pred,location_state,centerness):\n",
    "    with tf.variable_scope(\"loss/iou\") as scope:\n",
    "        # pos location\n",
    "        indices = tf.where(tf.equal(location_state, 1))\n",
    "    #     print(indices.shape)\n",
    "        if tf.size(indices) == 0:\n",
    "            return tf.constant(0.0)\n",
    "        y_regr_pred = tf.gather_nd(y_pred, indices)\n",
    "    #     print(y_regr_pred.shape)\n",
    "        y_regr_true = tf.gather_nd(y_true, indices)\n",
    "        y_centerness_true = tf.gather_nd(centerness,indices)\n",
    "\n",
    "        # (num_pos, )\n",
    "        pred_top = y_regr_pred[:, 0]\n",
    "        pred_bottom = y_regr_pred[:, 1]\n",
    "        pred_left = y_regr_pred[:, 2]\n",
    "        pred_right = y_regr_pred[:, 3]\n",
    "\n",
    "        # (num_pos, )\n",
    "        target_top = y_regr_true[:, 0]\n",
    "        target_bottom = y_regr_true[:, 1]\n",
    "        target_left = y_regr_true[:, 2]\n",
    "        target_right = y_regr_true[:, 3]\n",
    "\n",
    "        target_area = (target_left + target_right) * (target_top + target_bottom)\n",
    "        pred_area = (pred_left + pred_right) * (pred_top + pred_bottom)\n",
    "        w_intersect = tf.minimum(pred_left, target_left) + tf.minimum(pred_right, target_right)\n",
    "        h_intersect = tf.minimum(pred_bottom, target_bottom) + tf.minimum(pred_top, target_top)\n",
    "\n",
    "        g_w_intersect = tf.maximum(pred_left, target_left) + tf.maximum(pred_right, target_right)\n",
    "        g_h_intersect = tf.maximum(pred_bottom, target_bottom) + tf.maximum(pred_top, target_top)\n",
    "        ac_union = g_w_intersect * g_h_intersect\n",
    "        euclidean_distance = tf.pow((-target_left + target_right)/2 -(-pred_left + pred_right)/2,2) + tf.pow((-target_top + target_bottom)/2 -(-pred_top + pred_bottom)/2,2)\n",
    "        area_intersect = w_intersect * h_intersect\n",
    "        area_union = target_area + pred_area - area_intersect\n",
    "        ious = (area_intersect) / (area_union+1e-8)\n",
    "        dious = 1-ious+(euclidean_distance/tf.pow(ac_union+1e-8,2))\n",
    "        # (num_pos, )\n",
    "        #losses = tf.reduce_sum(losses * y_centerness_true) / (tf.reduce_sum(y_centerness_true) + 1e-6)\n",
    "\n",
    "        losses =  tf.reduce_mean(dious)\n",
    "    return losses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
